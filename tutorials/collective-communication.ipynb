{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-QlZE9mvdY"
      },
      "source": [
        "<!-- ---\n",
        "title: Collective Communication with Ignite\n",
        "weight: 5\n",
        "date: 2021-10-5\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - idist\n",
        "  - all_gather\n",
        "  - all_reduce\n",
        "  - broadcast\n",
        "  - barrier\n",
        "--- -->\n",
        "\n",
        "# Collective Communication with Ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgTaKWU8Doq"
      },
      "source": [
        "In this tutorial, we will see how to use advanced distributed functions like `all_reduce()`, `all_gather()`, `broadcast()` and `barrier()`. We will discuss unique use cases for all of them and represent them visually.\n",
        "\n",
        "<!--more-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhiy_ylcn2GD"
      },
      "source": [
        "## Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zevsoVQ4nx7"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrvIsRKQn42e"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lMphyBmmmvdw",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import ignite.distributed as idist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2KPTliNC2r9"
      },
      "source": [
        "## All Reduce\n",
        "\n",
        "![All Reduce Diagram](assets/all-reduce.png)\n",
        "\n",
        "The [`all_reduce()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.all_reduce) method is used to collect specified tensors from each process and make them available on every node then perform a specified operation (sum, product, min, max, etc) on them. For example - You need to find the average of all the gradients available on different processes. \n",
        "\n",
        "> First, we get the number of GPUs available, with the get_world_size method. Then, for every model parameter, we do the following:\n",
        ">\n",
        ">    1. Gather the gradients on each process\n",
        ">    2. Apply the sum operation on the gradients\n",
        ">    3. Divide by the world size to average them\n",
        ">\n",
        "> Finally, we can go on to update the model parameters using the averaged gradients!\n",
        ">\n",
        "> -- <cite>[Distributed Deep Learning 101: Introduction](https://towardsdatascience.com/distributed-deep-learning-101-introduction-ebfc1bcd59d9)</cite>\n",
        "\n",
        "You can get the number of GPUs (processes) available using another helper method [`idist.get_world_size()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_world_size) and then use `all_reduce()` to collect the gradients and apply the SUM operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j_ErUWhHpTl"
      },
      "outputs": [],
      "source": [
        "def average_gradients(model):\n",
        "    num_processes = idist.get_world_size()\n",
        "    for param in model.parameters():\n",
        "        idist.all_reduce(param.grad.data, op=\"SUM\")\n",
        "        param.grad.data = param.grad.data / num_processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w9oIcIiC6_4"
      },
      "source": [
        "## All Gather\n",
        "\n",
        "![All Gather Diagram](assets/all-gather.png)\n",
        "\n",
        "The [`all_gather()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.all_gather) method is used when you just want to collect a tensor, number or string across all participating processes. For example - You need to gather the predicted values which are distributed across all the processes on the main process so you could store them to a file. Here is how you can do it: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRvgasbLC8Ne"
      },
      "outputs": [],
      "source": [
        "def write_preds_to_file(predictions, filename):\n",
        "    prediction_tensor = torch.tensor(predictions)\n",
        "    prediction_tensor = idist.all_gather(prediction_tensor)\n",
        "\n",
        "    if idist.get_rank() == 0:\n",
        "        torch.save(prediction_tensor, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsu-NybC8t1"
      },
      "source": [
        "## Broadcast\n",
        "\n",
        "![Broadcast Diagram](assets/broadcast.png)\n",
        "\n",
        "The [`broadcast()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.broadcast) method copies a tensor, float or string from a source process to all the other processes. \n",
        "\n",
        "For example - You need to gather the predicted and actual values from all the processes on rank 0 for computing a metric and avoiding a memory error. You can do do this by first using `all_gather()`, then computing the metric and finally using `broadcast()` to share the result with all processes. `src` below refers to the rank of the source process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aXwKnrTC96T"
      },
      "outputs": [],
      "source": [
        "def compute_metric(prediction_tensor, target_tensor):\n",
        "\n",
        "    prediction_tensor = idist.all_gather(prediction_tensor)\n",
        "    target_tensor = idist.all_gather(target_tensor)\n",
        "\n",
        "    result = 0.0\n",
        "    if idist.get_rank() == 0:\n",
        "        result = compute_fn(prediction_tensor, target_tensor)\n",
        "\n",
        "    result = idist.broadcast(result, src=0)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ma7l5cDIuC"
      },
      "source": [
        "## Barrier\n",
        "\n",
        "The [`barrier()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.barrier) method helps synchronize all processes. For example - while downloading data during training, we have to make sure only the main process (`rank = 0`) downloads the datasets to prevent the sub processes (`rank > 0`) from downloading the same file to the same path at the same time. This way all sub processes get a copy of this already downloaded dataset. This is where we can utilize `barrier()` to make the sub processes wait until the main process downloads the datasets. Once that is done, all the subprocesses instantiate the datasets, while the main process waits. Finally, all the processes are synced up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XInr0zlhDJl6"
      },
      "outputs": [],
      "source": [
        "def get_datasets(config):\n",
        "    if idist.get_local_rank() > 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    train_dataset, test_dataset = get_train_test_datasets(config[\"data_path\"])\n",
        "\n",
        "    if idist.get_local_rank() == 0:\n",
        "        idist.barrier()\n",
        "\n",
        "    return train_dataset, test_dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "idist-collective-communication.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
